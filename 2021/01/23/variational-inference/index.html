<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>ozan öğreden</title> <meta name="description" content=""> <meta name="keywords" content="Ozan Öğreden, personal homepage"> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <!-- Social: Facebook / Open Graph --> <meta property="og:type" content="article"> <meta property="article:author" content="Ozan Öğreden"> <meta property="article:section" content="writing"> <meta property="article:tag" content=""> <meta property="article:published_time" content="2021-01-23 00:00:00 +0100"> <meta property="og:url" content="https://ozan.ogreden.com/2021/01/23/variational-inference/"> <meta property="og:title" content=""> <meta property="og:image" content="https://ozan.ogreden.com"> <meta property="og:description" content=""> <meta property="og:site_name" content="Ozan Öğreden"> <meta property="og:locale" content="en_US"> <!-- Social: Twitter --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content="ozanogred_en"> <meta name="twitter:title" content=""> <meta name="twitter:description" content=""> <meta name="twitter:image:src" content="https://ozan.ogreden.com"> <!-- Social: Google+ / Schema.org --> <meta itemprop="name" content=""> <meta itemprop="description" content=""> <meta itemprop="image" content="https://ozan.ogreden.com"> <!-- Canonical link tag --> <!-- <meta http-equiv="refresh" content="0; url=/2021/01/23/variational-inference/"> <link rel="canonical" href="/2021/01/23/variational-inference/"> --> <link rel="canonical" href="https://ozan.ogreden.com/2021/01/23/variational-inference/"> <link rel="alternate" type="application/rss+xml" title="ozan öğreden" href="https://ozan.ogreden.com/feed.xml"> <!-- rel prev and next --> <link rel="stylesheet" href="/assets/css/styles.css"> <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" /> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-mml-chtml.js"> </script> </head> <body> <main> <div id="header-container"> <header class="site-header"> <h1 id="header-title"> <a href="/">ozan öğreden</a> </h1> </header> <nav> <div style="padding-top: 1em"> <ul> </ul> </div> </nav> </div> <article class="post-container" itemscope itemtype="http://schema.org/BlogPosting"> <!-- don't show titles on zettels --> <header class="post-header"> <p class="post-meta"><time datetime="2021-01-23T00:00:00+01:00" itemprop="datePublished">Jan 23, 2021</time></p> <h2 id="post-title" class="post-title" itemprop="name headline">Basics of variational inference - theory</h1> </header> <div class="post-content" itemprop="articleBody"> <p>Switching to a Bayesian workflow can be frustrating initially, when the sampling speed becomes limiting and you have no idea why. Like it happened to me <a href="/2021/01/08/dirichlet-processes-label-switching/">when I tried to fit a Dirichlet process mixture model</a> <!-- [[202012161651 dirichlet process post]] --> using PyMC3. Any attempt at working around the difficulties I ran into got blocked by the speed of NUTS sampler.</p> <p>It turns out that variational inference (VI) can help in these stages. However, it seems to have its own peculiarities and even some pitfalls. Not having learned about VI during my master’s, I set out to read about it.</p> <p>VI is a method for approximating probability densities<!-- [[202101121750 Variational Inference A Review For Statisticians]] -->. In the context of Bayesian modeling, we can use it to approximate the posterior. As such, we can consider it an alternative to markov chain monte carlo (MCMC) algorithms. There is one important advantage of VI: it’s very fast compared to MCMC. And there is an important disadvantage: you don’t get to be sure that the approximation will give you what you’re looking for, unlike with MCMC.</p> <p>Let’s start with remembering what we’re looking for in Bayesian inference problem: the posterior, or the density of model parameters conditional on the data. Mathematically speaking:</p> \[p(z | x)\] <p>Commonly used modern methods for getting to the posterior are based on sampling, a.k.a markov chain monte carlo algorithms. VI, on the other hand, approaches the problem from an optimisation point of view, by making use of a clever trick. Before we get to the trick, let’s specify the objective of the optimisation: Kullbeck-Leibler (KL) divergence <!-- [[202101121805 Kullbeck-Leibler divergence]] -->.</p> <p>The Kullbeck-Leibler divergence is a measure if the “difference in information contained within two distributions” <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. In the VI context, specifically, the KL divergence from the posterior to the approximating density is relevant: \(KL( q(\mathbf{z}) \| p(\mathbf{z} \vert \mathbf{x} ) )\)</p> <p>Buried in this divergence is a term that’s very difficult to evaluate: \(p(\boldsymbol{x})\). The trick is to optimise for another objective which is (1) easier to evaluate and (2) is equivalent to minimising the KL divergence: Evidence lower bound (ELBO).</p> \[ELBO(q) = \mathbb{E}[logp(\mathbf{z}, \mathbf{x})] - \mathbb{E}[logq(\mathbf{z})]\] <p>Blei et al. (2018) note that “the variational objective mirrors the usual balance between likelihood and prior” <sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> <!-- [[202101121750 Variational Inference A Review For Statisticians]] --> and this is visible once the identity is rearranged:</p> \[ELBO(q) = \mathbb{E}[logp(\mathbf{x}|\mathbf{z})] + KL(q(\mathbf{z})||p(\mathbf{z}))\] <p>Great. But how do we specify \(q\)? A commonly used family of probability distributions is “mean-field variational family”. The mean-field variational family is essentially a joint distribution of independent random variables. So we have:</p> \[q(\boldsymbol{z}) = \prod_{j=1}^{m}q_j(z_j).\] <p>This makes one of the key properties of VI with mean-field family clear: <strong>we’re assuming that model parameters are mutually independent</strong>. If they are correlated, the approximation will not be able to capture that information.</p> <p>Now, we need an optimisation algorithm and we’re ready to go. I’ll refer you to Blei et al. (2018) <sup id="fnref:2:1"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> for the details, where you can read about coordinate ascent mean-field variational inference.</p> <p>Now let’s turn to what’s happening when we use the <a href="https://docs.pymc.io/notebooks/variational_api_quickstart.html">variational API of PyMC3</a>. <a href="https://github.com/pymc-devs/pymc3/blob/aedc8e9006c7e672e461774a5ee095562405c99c/pymc3/variational/inference.py#L732-L741">Here</a> you can see that the default of the <code class="language-plaintext highlighter-rouge">method=</code> parameter is ADVI (automatic differentiation variational inference). ADVI is an algorithm that can express a given probabilistic model as an optimisation problem. The significant advantage of it is that we don’t have to think about technical requirements that we need to satisfy to make the approximation work. Any model we can write in PyMC3 (also Stan) can be run through ADVI.</p> <p>Equipped with the magic provided by PyMC3 and with a basic understanding of variational inference. It’s time to see it in practice!</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>https://ermongroup.github.io/cs228-notes/inference/variational/ <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>https://arxiv.org/abs/1601.00670 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> </ol> </div> </div> </article> </main> </body> </html>
